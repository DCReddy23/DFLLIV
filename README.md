# DFLLIV - Diffusion Fields for Low-Light Image and Video Enhancement

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DCReddy23/DFLLIV/blob/main/notebooks/DFLLIV_Colab_Training.ipynb)

A state-of-the-art low-light image enhancement framework using **Diffusion Fields** - combining denoising diffusion probabilistic models (DDPMs) with neural fields (implicit neural representations).

## ğŸš€ Quick Start with Google Colab

**Want to train without any local setup?** Click the badge above to open our Google Colab notebook!

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DCReddy23/DFLLIV/blob/main/notebooks/DFLLIV_Colab_Training.ipynb)

The notebook includes:
- âœ… Complete environment setup
- âœ… LOL dataset download and preparation
- âœ… Training with Colab's free GPU (T4)
- âœ… Inference and evaluation examples
- âœ… Results visualization

**Training Time on Colab:** ~4-6 hours for 100 epochs (T4 GPU)

## ğŸŒŸ Overview

Traditional low-light enhancement methods often struggle with preserving details while reducing noise and improving brightness. This project introduces a novel approach by:

- **Modeling images as continuous functions** using neural fields instead of discrete pixel grids
- **Applying diffusion processes** in the continuous function space for superior quality
- **Supporting both diffusion field and UNet architectures** for flexibility
- **Providing complete training, inference, and evaluation pipelines**

### Why Diffusion Fields?

1. **Continuous representation**: Better interpolation and detail preservation
2. **Powerful generative modeling**: Leverages the success of diffusion models
3. **Noise robustness**: Natural handling of low-light noise through the diffusion process
4. **High-quality results**: State-of-the-art performance on standard benchmarks

## ğŸ—ï¸ Architecture

The pipeline consists of:

```
Low-Light Image
      â†“
Condition Encoder (ResNet-18) â†’ Conditioning Vector (256-dim)
      â†“                                    â†“
Coordinate Grid (HÃ—WÃ—2) â†’ Fourier Encoding â†’ Diffusion Field MLP
      â†“                                    â†“
Timestep â†’ Sinusoidal Embedding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
      â†“
Noise Prediction (Îµ)
      â†“
DDPM/DDIM Sampling
      â†“
Enhanced Image
```

### Key Components

1. **Coordinate Encoder** (`models/coord_encoder.py`)
   - Fourier feature encoding with 128 frequencies
   - Transforms (x, y) coordinates into high-dimensional features
   - Enables learning of high-frequency details

2. **Condition Encoder** (`models/condition_encoder.py`)
   - ResNet-18 backbone (pretrained on ImageNet)
   - Extracts global context from low-light input
   - Outputs 256-dimensional conditioning vector

3. **Diffusion Field MLP** (`models/diffusion_field.py`)
   - 8-layer MLP with 256 hidden units
   - Skip connections (NeRF-inspired)
   - SiLU/Swish activations
   - Takes: coordinates, condition, timestep â†’ predicts noise

4. **Noise Scheduler** (`models/noise_scheduler.py`)
   - Supports linear and cosine beta schedules
   - DDPM and DDIM sampling
   - 1000 timesteps (configurable)

5. **UNet Alternative** (`models/unet.py`)
   - Standard UNet architecture for pixel-space diffusion
   - Encoder-decoder with skip connections
   - Time and condition embedding injection

## âœ¨ Features

- âœ… Complete training pipeline with mixed precision (AMP)
- âœ… Exponential Moving Average (EMA) of weights
- âœ… Configurable learning rate scheduling with warmup
- âœ… TensorBoard and Weights & Biases logging
- âœ… DDPM and DDIM sampling (fast inference with 50 steps)
- âœ… Comprehensive evaluation metrics (PSNR, SSIM, LPIPS)
- âœ… Multiple dataset support (LOL, LOL-v2, synthetic)
- âœ… Synthetic low-light pair generation
- âœ… Side-by-side comparison visualizations
- âœ… Resume training from checkpoints
- âœ… Batch inference support

## ğŸ“¦ Installation

### Requirements

- Python 3.8+
- CUDA-capable GPU (recommended, but CPU also supported)
- 8GB+ GPU memory for training

### Setup

```bash
# Clone the repository
git clone https://github.com/DCReddy23/DFLLIV.git
cd DFLLIV

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“Š Dataset Setup

### LOL Dataset (Recommended)

The LOL (Low-Light) dataset contains 500 paired low/normal-light images.

**Step 1:** Run the download script
```bash
cd data
./download_lol.sh
```

**Step 2:** Manual download (if needed)
1. Visit: https://drive.google.com/file/d/157bjO1_cFuSd0HWDUuAmcHRJDVyWpOxB/view
2. Download `LOLdataset.zip`
3. Place it in `data/` directory
4. Run `./download_lol.sh` again to extract

**Expected structure:**
```
data/LOL/
â”œâ”€â”€ our485/          # Training set (485 pairs)
â”‚   â”œâ”€â”€ low/
â”‚   â””â”€â”€ high/
â””â”€â”€ eval15/          # Test set (15 pairs)
    â”œâ”€â”€ low/
    â””â”€â”€ high/
```

### LOL-v2 Dataset (Optional)

For extended training with 1000+ pairs:
1. Visit: https://github.com/flyywh/CVPR-2020-Semi-Low-Light
2. Download LOL-v2 (Real and Synthetic subsets)
3. Extract to `data/LOL-v2/`

See [data/README.md](data/README.md) for detailed dataset documentation.

### Synthetic Data Generation

Generate low-light images from any well-lit dataset:

```python
from data.dataset import SyntheticLowLightDataset

dataset = SyntheticLowLightDataset(
    image_dir='path/to/images',
    gamma_range=(2.0, 5.0),
    brightness_range=(0.3, 0.7)
)
```

## ğŸš€ Quick Start

### 1. Train a Model

```bash
python train.py --config configs/default.yaml
```

**Key arguments:**
- `--config`: Path to configuration file
- `--resume`: Resume from checkpoint

**Multi-GPU training:**
```bash
CUDA_VISIBLE_DEVICES=0,1 python train.py --config configs/default.yaml
```

### 2. Enhance Images

**Single image:**
```bash
python inference.py \
    --checkpoint checkpoints/best.pth \
    --input examples/low_light.jpg \
    --output results/enhanced.png \
    --num-steps 50
```

**Batch processing:**
```bash
python inference.py \
    --checkpoint checkpoints/best.pth \
    --input data/test_images/ \
    --output results/ \
    --num-steps 50
```

### 3. Evaluate Performance

```bash
python evaluate.py \
    --checkpoint checkpoints/best.pth \
    --dataset-dir data/LOL/eval15 \
    --output-dir results/eval \
    --num-steps 50
```

## ğŸ“ Configuration

All hyperparameters are in `configs/default.yaml`:

```yaml
model:
  type: "diffusion_field"  # or "unet"
  hidden_dim: 256
  num_layers: 8
  fourier_frequencies: 128

diffusion:
  num_timesteps: 1000
  beta_schedule: "cosine"
  sampling_method: "ddim"
  ddim_steps: 50

training:
  batch_size: 8
  learning_rate: 1e-4
  num_epochs: 500
  grad_clip: 1.0
  ema_decay: 0.9999
```

See the [configuration file](configs/default.yaml) for all options.

## ğŸ“ˆ Training

### Monitor Training

**TensorBoard:**
```bash
tensorboard --logdir runs/
```

**Weights & Biases** (optional):
```bash
wandb login
# Training will automatically log to W&B
```

### Checkpoints

Checkpoints are saved in `checkpoints/`:
- `latest.pth`: Most recent epoch
- `best.pth`: Best validation PSNR
- `checkpoint_epoch_N.pth`: Periodic saves

### Resume Training

```bash
python train.py --config configs/default.yaml --resume checkpoints/latest.pth
```

## ğŸ¯ Inference Options

### Sampling Methods

**DDIM (Fast, Recommended):**
- 50 steps (default)
- Deterministic when eta=0.0
- ~5-10 seconds per image

```bash
python inference.py --checkpoint checkpoints/best.pth \
    --input test.jpg --output enhanced.png \
    --sampling-method ddim --num-steps 50 --eta 0.0
```

**DDPM (High Quality):**
- 1000 steps
- Stochastic sampling
- ~2-3 minutes per image

```bash
python inference.py --checkpoint checkpoints/best.pth \
    --input test.jpg --output enhanced.png \
    --sampling-method ddpm
```

### Output Formats

By default, outputs include side-by-side comparisons. For enhanced image only:
```bash
python inference.py ... --no-comparison
```

## ğŸ“Š Evaluation Metrics

The evaluation script computes:

- **PSNR** (Peak Signal-to-Noise Ratio): Higher is better
- **SSIM** (Structural Similarity Index): Closer to 1 is better
- **LPIPS** (Learned Perceptual Image Patch Similarity): Lower is better

Results are saved to:
- `results.csv`: Per-image metrics
- `metrics.json`: Average metrics with standard deviations
- `comparison_grid.png`: Visual comparison of 8 samples

## ğŸ“‚ Project Structure

```
DFLLIV/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml             # Default hyperparameters
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py               # Dataset loaders
â”‚   â”œâ”€â”€ download_lol.sh          # LOL dataset download script
â”‚   â””â”€â”€ README.md                # Dataset documentation
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ coord_encoder.py         # Fourier coordinate encoding
â”‚   â”œâ”€â”€ condition_encoder.py     # ResNet-18 conditioning
â”‚   â”œâ”€â”€ diffusion_field.py       # Core diffusion field MLP
â”‚   â”œâ”€â”€ noise_scheduler.py       # DDPM/DDIM scheduling
â”‚   â””â”€â”€ unet.py                  # UNet architecture
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py               # PSNR, SSIM, LPIPS
â”‚   â””â”€â”€ visualization.py         # Plotting utilities
â”‚
â”œâ”€â”€ train.py                     # Training script
â”œâ”€â”€ inference.py                 # Image enhancement script
â””â”€â”€ evaluate.py                  # Evaluation script
```

## ğŸ”¬ Results

### Expected Performance on LOL Dataset

| Model | PSNR â†‘ | SSIM â†‘ | LPIPS â†“ |
|-------|--------|--------|---------|
| Diffusion Field (Ours) | TBD | TBD | TBD |
| UNet Baseline | TBD | TBD | TBD |

*Note: Results will be updated after training completion.*

### Qualitative Results

Example enhancements will be added here after training.

## ğŸ› ï¸ Troubleshooting

### Common Issues

**Out of memory:**
- Reduce `batch_size` in config
- Use gradient accumulation
- Reduce `crop_size` to 128 or 192

**Slow training:**
- Ensure CUDA is available: `torch.cuda.is_available()`
- Use DDIM sampling for faster validation
- Reduce validation frequency (`val_every`)

**NaN losses:**
- Reduce learning rate
- Check gradient clipping value
- Ensure proper data normalization

### FAQ

**Q: How long does training take?**
A: On a single RTX 3090, expect ~24-48 hours for 500 epochs on LOL dataset.

**Q: Can I train without a GPU?**
A: Yes, but it will be very slow. GPU is strongly recommended.

**Q: How do I use my own dataset?**
A: Organize your data in the same structure as LOL (low/ and high/ directories), or use the `SyntheticLowLightDataset` class.

**Q: What's the difference between diffusion field and UNet?**
A: Diffusion fields model images as continuous functions (better for details), while UNet operates on pixel grids (faster training). Try both!

## ğŸ“š Citation

If you use this code, please cite the relevant papers:

```bibtex
@inproceedings{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{song2020denoising,
  title={Denoising Diffusion Implicit Models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{mildenhall2020nerf,
  title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{wei2018deep,
  title={Deep retinex decomposition for low-light enhancement},
  author={Wei, Chen and Wang, Wenjing and Yang, Wenhan and Liu, Jiaying},
  booktitle={BMVC},
  year={2018}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ™ Acknowledgments

- LOL dataset by Wei et al.
- DDPM/DDIM implementations inspired by HuggingFace Diffusers
- NeRF architecture design by Mildenhall et al.

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub.

---

**Made with â¤ï¸ for better low-light image enhancement**
